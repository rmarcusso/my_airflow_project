{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pyself. Resources\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# My Imports\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# PySpark Imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class MyFunctions(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.spark = (SparkSession.builder\n",
    "                      .config('spark.jars', 'driver/postgresql-42.6.0.jar')\n",
    "                      .config('spark.driver.extraClassPath', 'driver/postgresql-42.6.0.jar')\n",
    "                      .appName(\"MyProject\").getOrCreate())\n",
    "\n",
    "        self.host = \"localhost\"\n",
    "        self.port = \"5432\"\n",
    "        self.database = \"ensurwave\"\n",
    "        self.username = \"postgres\"\n",
    "        self.password = \"postgres\"\n",
    "        self.url = f\"jdbc:postgresql://{self.host}:{self.port}/{self.database}\"\n",
    "\n",
    "        self.file = 'data/new/20230331_employees_details.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 23:46:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 23:46:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "mf = MyFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+-------------------+\n",
      "| id|              schema|             version|            fileName|     load_timestamp|\n",
      "+---+--------------------+--------------------+--------------------+-------------------+\n",
      "|  1|StructType([Struc...|f7a43c8956169f623...|20230331_employee...|2023-04-15 23:46:18|\n",
      "+---+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the schema\n",
    "schema_raw_data = str(mf.spark.read.option('inferSchema',True).option('multiline','true').json(mf.file).schema)\n",
    "\n",
    "current_no_space = schema_raw_data.replace(' ','')\n",
    "current_no_space = [i for i in current_no_space]\n",
    "current_no_space.sort()\n",
    "current_no_space = ''.join(current_no_space)\n",
    "\n",
    "current_hash_no_space = hashlib.sha256(current_no_space.encode('utf-8'))\n",
    "current_hash_no_space = current_hash_no_space.hexdigest()\n",
    "\n",
    "# Mounting the dataframe to save the history of schemas used.\n",
    "data = [{'id' : 1, 'schema' : schema_raw_data, 'version' : current_hash_no_space, 'fileName':mf.file.split('/')[-1]}]\n",
    "current_schema = mf.spark.createDataFrame(data)\n",
    "current_schema = current_schema.select('id','schema','version','fileName').withColumn('load_timestamp', to_timestamp(lit(datetime.now().strftime(\"%Y%m%d %H:%M:%S\")),'yyyyMMdd H:m:s'))\n",
    "current_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nova Tabela\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get Current table in \n",
    "try:\n",
    "    table_history = (mf.spark.read.format('jdbc').option('url', mf.url)\n",
    "                        .option('dbtable', 'schema_history')\n",
    "                        .option('user', mf.username)\n",
    "                        .option('password', mf.password)\n",
    "                        .option('driver', 'org.postgresql.Driver').load())\n",
    "\n",
    "    max_id = table_history.select(max(col('id')).alias('max_id')).collect()[0][0]\n",
    "    new_id = max_id + 1\n",
    "\n",
    "    new_value = [{'id':new_id, 'schema':schema_raw_data, 'version' : current_hash_no_space, 'fileName':mf.file.split('/')[-1]}]\n",
    "\n",
    "    df_new_value = mf.spark.createDataFrame(new_value)\n",
    "    df_new_value = df_new_value.select('id','schema','version','fileName').withColumn('load_timestamp', to_timestamp(lit(datetime.now().strftime(\"%Y%m%d %H:%M:%S\")),'yyyyMMdd H:m:s'))\n",
    "\n",
    "    old_hash_schema = table_history.filter(f\"id == ({max_id})\").select('version').collect()[0][0]\n",
    "    \n",
    "    if old_hash_schema != current_hash_no_space:\n",
    "        df_persist = table_history.unionAll(df_new_value)\n",
    "\n",
    "        df_persist.write.mode('overwrite').saveAsTable('df_persist')\n",
    "\n",
    "        tb_persist = mf.spark.table('df_persist').orderBy(col('id'))\n",
    "\n",
    "        (tb_persist\n",
    "            .write\n",
    "            .mode('overwrite')\n",
    "            .format('jdbc')\n",
    "            .option('url', mf.url)\n",
    "            .option('dbtable', 'schema_history')\n",
    "            .option('user', mf.username)\n",
    "            .option('password', mf.password)\n",
    "            .option('driver', 'org.postgresql.Driver')\n",
    "            .save())\n",
    "        shutil.rmtree(f'{os.getcwd()}/spark-warehouse')\n",
    "# # Creating Table\n",
    "except:\n",
    "    print('Nova Tabela')\n",
    "    (current_schema\n",
    "        .write\n",
    "        .format('jdbc')\n",
    "        .option('url', mf.url)\n",
    "        .mode('overwrite')\n",
    "        .option('dbtable', 'schema_history')\n",
    "        .option('user', mf.username)\n",
    "        .option('password', mf.password)\n",
    "        .option('driver', 'org.postgresql.Driver')\n",
    "        .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+-------------------+\n",
      "| id|              schema|             version|            fileName|     load_timestamp|\n",
      "+---+--------------------+--------------------+--------------------+-------------------+\n",
      "|  1|StructType([Struc...|f7a43c8956169f623...|20230331_employee...|2023-04-15 23:46:18|\n",
      "+---+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current = (mf.spark.read\n",
    "    .format('jdbc')\n",
    "    .option('url', mf.url)\n",
    "    .option('dbtable', 'schema_history')\n",
    "    .option('user', mf.username)\n",
    "    .option('password', mf.password)\n",
    "    .option('driver', 'org.postgresql.Driver')\n",
    "    .load())\n",
    "current.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
