{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pyself. Resources\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# My Imports\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# PySpark Imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class MyFunctions(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.spark = (SparkSession.builder\n",
    "                      .config('spark.jars', 'driver/postgresql-42.6.0.jar')\n",
    "                      .config('spark.driver.extraClassPath', 'driver/postgresql-42.6.0.jar')\n",
    "                      .appName(\"MyProject\").getOrCreate())\n",
    "\n",
    "        self.host = \"localhost\"\n",
    "        self.port = \"5432\"\n",
    "        self.database = \"ensurwave\"\n",
    "        self.username = \"postgres\"\n",
    "        self.password = \"postgres\"\n",
    "        self.url = f\"jdbc:postgresql://{self.host}:{self.port}/{self.database}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-04-13 12:50:33'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_date=datetime.now().strptime(datetime.now().strftime(\"%Y%m%d\"),\"%Y%m%d\")\n",
    "load_date=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "load_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Marcusso/Documents/git/my_airflow_project/include'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MyFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+---------+--------------------+\n",
      "|          attributes|        id|isDeleted|     name|        salaryValues|\n",
      "+--------------------+----------+---------+---------+--------------------+\n",
      "|{2023-02-15T15:09...|abd1234rty|    false|Bob Smith|[{USD, Base, 5676...|\n",
      "+--------------------+----------+---------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a PySpark dataframe\n",
    "employees_raw = mf.spark.read.option('inferSchema', True).option(\n",
    "    'multiline', 'true').json('data/new/20230331_employees_details.json')\n",
    "\n",
    "employees_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+--------------------+--------+----------------+-----+-----+-------------------+\n",
      "|        id|isDeleted|     name|            joinedOn|position|satisfactionScoe| Base|Bonus|           loaddate|\n",
      "+----------+---------+---------+--------------------+--------+----------------+-----+-----+-------------------+\n",
      "|abd1234rty|    false|Bob Smith|2023-02-15T15:09:...| Manager|            10.5|56767| 5000|2023-04-13 12:52:46|\n",
      "+----------+---------+---------+--------------------+--------+----------------+-----+-----+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[id: string, isDeleted: boolean, name: string, joinedOn: string, position: string, satisfactionScoe: double, Base: bigint, Bonus: bigint, loaddate: timestamp]>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "other_types = [column for column, datatype in employees_raw.dtypes if 'struct' not in datatype and 'array' not in datatype]\n",
    "struct_types = [f'{column}.*' for column, datatype in employees_raw.dtypes if 'struct' in datatype and 'array' not in datatype]\n",
    "array_types = [column for column, datatype in employees_raw.dtypes if 'array' in datatype]\n",
    "\n",
    "# Columns separating structs and array types\n",
    "new = other_types + struct_types + array_types\n",
    "\n",
    "second_change = employees_raw.select(new)\n",
    "\n",
    "for i in array_types:\n",
    "    second_change = second_change.select('*',explode(i).alias(f'{i}_ex')).drop(i).withColumnRenamed(f'{i}_ex',i)\n",
    "    second_change = second_change.select('*',f'{i}.*').drop(i)\n",
    "\n",
    "salary = second_change.select(col('id').alias('id_emp'),'currency','type','value').groupBy('id_emp','currency').pivot('type').sum('value').drop('value','type').dropDuplicates()\n",
    "\n",
    "last_change = second_change.alias('emp').join(salary.alias('sal'), on=col('emp.id')==col('sal.id_emp'), how='inner').drop('id_emp','currency','type','value').dropDuplicates()\n",
    "last_change = last_change.withColumn('loaddate', to_timestamp(lit(datetime.now().strftime(\"%Y%m%d %H:%M:%S\")),'yyyyMMdd H:m:s'))\n",
    "last_change.show()\n",
    "last_change.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (last_change\n",
    "#     .write\n",
    "#     .format('jdbc')\n",
    "#     .option('url', mf.url)\n",
    "#     .mode('overwrite')\n",
    "#     .option('dbtable', 'employee')\n",
    "#     .option('user', mf.username)\n",
    "#     .option('password', mf.password)\n",
    "#     .option('driver', 'org.postgresql.Driver')\n",
    "#     .save())\n",
    "\n",
    "# Pare a sess√£o Spark\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|              schema|version|\n",
      "+--------------------+-------+\n",
      "|StructType([Struc...|      1|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(mf.spark.read.format('jdbc').option('url', mf.url)\n",
    "    .option('dbtable', 'schema_version')\n",
    "    .option('user', mf.username)\n",
    "    .option('password', mf.password)\n",
    "    .option('driver', 'org.postgresql.Driver').load().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
