{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pyself. Resources\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# My Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# PySpark Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "class MyFunctions(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.spark = (SparkSession.builder\n",
    "                      .config('spark.jars', 'driver/postgresql-42.6.0.jar')\n",
    "                      .config('spark.driver.extraClassPath', 'driver/postgresql-42.6.0.jar')\n",
    "                      .appName(\"MyProject\").getOrCreate())\n",
    "\n",
    "        self.host = \"localhost\"\n",
    "        self.port = \"5432\"\n",
    "        self.database = \"ensurwave\"\n",
    "        self.username = \"postgres\"\n",
    "        self.password = \"postgres\"\n",
    "        self.url = f\"jdbc:postgresql://{self.host}:{self.port}/{self.database}\"\n",
    "\n",
    "        self.file = 'data/new/20230331_employees_details.json'\n",
    "        self.file1 = 'data/new/20230417_employees_details.json'\n",
    "    \n",
    "    def rmtree_spark_directory(self):\n",
    "        try:\n",
    "            shutil.rmtree(f'{os.getcwd()}/spark-warehouse')\n",
    "        except:\n",
    "            print(f\"'{os.getcwd()}/spark-warehouse' doesn\\'t exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MyFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "verdade\n"
     ]
    }
   ],
   "source": [
    "is_exist_table = bool(mf.spark.read.format('jdbc')\n",
    "                .option('url', mf.url)\n",
    "                .option('dbtable', 'information_schema.tables')\n",
    "                .option('user', mf.username)\n",
    "                .option('password', mf.password)\n",
    "                .option('driver', 'org.postgresql.Driver').load().filter('table_name == \"schema_history\"').count())\n",
    "\n",
    "print(is_exist_table)\n",
    "\n",
    "if not is_exist_table:\n",
    "    print('verdade')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga do arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------+------+---------+-------------+----------------------------------------------------------------------------------------------+\n",
      "|attributes                                                                                               |id    |isDeleted|name         |salaryValues                                                                                  |\n",
      "+---------------------------------------------------------------------------------------------------------+------+---------+-------------+----------------------------------------------------------------------------------------------+\n",
      "|{Engineering, alice.johnson@example.com, Senior Developer, 2021-05-01T08:30:00.000Z, Active}             |abc123|false    |Alice Johnson|[{USD, Base, 100000}, {USD, Bonus, 15000}, {USD, Benefits, 5000}, {USD, Stock Options, 20000}]|\n",
      "|{Sales, john.doe@example.com, Sales Manager, 2020-01-01T09:00:00.000Z, Active}                           |def456|false    |John Doe     |[{USD, Base, 90000}, {USD, Bonus, 12000}, {USD, Benefits, 4000}, {USD, Commission, 30000}]    |\n",
      "|{Human Resources, sarah.lee@example.com, Human Resources Coordinator, 2019-08-01T08:30:00.000Z, Inactive}|ghi789|true     |Sarah Lee    |[{USD, Base, 75000}, {USD, Bonus, 8000}, {USD, Benefits, 3500}]                               |\n",
      "+---------------------------------------------------------------------------------------------------------+------+---------+-------------+----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a PySpark dataframe\n",
    "employees_raw = mf.spark.read.option('inferSchema', True).option(\n",
    "    'multiline', 'true').json(mf.file1)\n",
    "\n",
    "fileName = mf.file.split('/')[-1]\n",
    "\n",
    "load_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "employees_raw.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento das colunas existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+--------------------+--------------------+------+---------+-------------------+-------------+--------------------+--------------------+--------+-------------+------+\n",
      "|currency|     department|               email|            fileName|    id|isDeleted|     load_timestamp|         name|            position|           startDate|  status|         type| value|\n",
      "+--------+---------------+--------------------+--------------------+------+---------+-------------------+-------------+--------------------+--------------------+--------+-------------+------+\n",
      "|     USD|    Engineering|alice.johnson@exa...|20230331_employee...|abc123|    false|2023-04-17 19:48:28|Alice Johnson|    Senior Developer|2021-05-01T08:30:...|  Active|         Base|100000|\n",
      "|     USD|    Engineering|alice.johnson@exa...|20230331_employee...|abc123|    false|2023-04-17 19:48:28|Alice Johnson|    Senior Developer|2021-05-01T08:30:...|  Active|        Bonus| 15000|\n",
      "|     USD|    Engineering|alice.johnson@exa...|20230331_employee...|abc123|    false|2023-04-17 19:48:28|Alice Johnson|    Senior Developer|2021-05-01T08:30:...|  Active|     Benefits|  5000|\n",
      "|     USD|    Engineering|alice.johnson@exa...|20230331_employee...|abc123|    false|2023-04-17 19:48:28|Alice Johnson|    Senior Developer|2021-05-01T08:30:...|  Active|Stock Options| 20000|\n",
      "|     USD|          Sales|john.doe@example.com|20230331_employee...|def456|    false|2023-04-17 19:48:28|     John Doe|       Sales Manager|2020-01-01T09:00:...|  Active|         Base| 90000|\n",
      "|     USD|          Sales|john.doe@example.com|20230331_employee...|def456|    false|2023-04-17 19:48:28|     John Doe|       Sales Manager|2020-01-01T09:00:...|  Active|        Bonus| 12000|\n",
      "|     USD|          Sales|john.doe@example.com|20230331_employee...|def456|    false|2023-04-17 19:48:28|     John Doe|       Sales Manager|2020-01-01T09:00:...|  Active|     Benefits|  4000|\n",
      "|     USD|          Sales|john.doe@example.com|20230331_employee...|def456|    false|2023-04-17 19:48:28|     John Doe|       Sales Manager|2020-01-01T09:00:...|  Active|   Commission| 30000|\n",
      "|     USD|Human Resources|sarah.lee@example...|20230331_employee...|ghi789|     true|2023-04-17 19:48:28|    Sarah Lee|Human Resources C...|2019-08-01T08:30:...|Inactive|         Base| 75000|\n",
      "|     USD|Human Resources|sarah.lee@example...|20230331_employee...|ghi789|     true|2023-04-17 19:48:28|    Sarah Lee|Human Resources C...|2019-08-01T08:30:...|Inactive|        Bonus|  8000|\n",
      "|     USD|Human Resources|sarah.lee@example...|20230331_employee...|ghi789|     true|2023-04-17 19:48:28|    Sarah Lee|Human Resources C...|2019-08-01T08:30:...|Inactive|     Benefits|  3500|\n",
      "+--------+---------------+--------------------+--------------------+------+---------+-------------------+-------------+--------------------+--------------------+--------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "other_types = [column for column, datatype in employees_raw.dtypes if 'struct' not in datatype and 'array' not in datatype]\n",
    "struct_types = [f'{column}.*' for column, datatype in employees_raw.dtypes if 'struct' in datatype and 'array' not in datatype]\n",
    "array_types = [column for column, datatype in employees_raw.dtypes if 'array' in datatype]\n",
    "\n",
    "# Columns separating structs and array types\n",
    "new = other_types + struct_types + array_types\n",
    "\n",
    "second_change = employees_raw.select(new)\n",
    "\n",
    "for i in array_types:\n",
    "    second_change = second_change.select('*',explode(i).alias(f'{i}_ex')).drop(i).withColumnRenamed(f'{i}_ex',i)\n",
    "    second_change = second_change.select('*',f'{i}.*').drop(i)\n",
    "\n",
    "new_data = second_change.withColumn('fileName',lit(fileName)).withColumn('load_timestamp', to_timestamp(lit(load_date), 'yyyy-MM-dd HH:mm:ss'))\n",
    "new_dt_columns = new_data.columns\n",
    "new_dt_columns.sort()\n",
    "\n",
    "new_data = new_data.select(new_dt_columns)\n",
    "new_data.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captura das colunas que vieram do arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(currency, StringType())</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(department, StringType())</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(email, StringType())</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(fileName, StringType())</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(id, StringType())</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name\n",
       "0    (currency, StringType())\n",
       "1  (department, StringType())\n",
       "2       (email, StringType())\n",
       "3    (fileName, StringType())\n",
       "4          (id, StringType())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_columns = new_data.schema\n",
    "file_columns = [(i.name, i.dataType) for i in file_columns]\n",
    "file_columns.sort()\n",
    "\n",
    "file_columns = {\"name\":file_columns}\n",
    "df_file_columns = pd.DataFrame(file_columns)\n",
    "display(df_file_columns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/Users/Marcusso/Documents/git/my_airflow_project/include/spark-warehouse' doesn't exists.\n"
     ]
    }
   ],
   "source": [
    "mf.rmtree_spark_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisting data...\n"
     ]
    }
   ],
   "source": [
    "# Evaluating inf the table exists\n",
    "is_exist_table = (mf.spark.read.format('jdbc')\n",
    "         .option('url', mf.url)\n",
    "         .option('dbtable', 'information_schema.tables')\n",
    "         .option('user', mf.username)\n",
    "         .option('password', mf.password)\n",
    "         .option('driver', 'org.postgresql.Driver').load().filter('table_name == \"employees\"').count())\n",
    "\n",
    "# Creating table if not exists\n",
    "if not is_exist_table:\n",
    "    print('Creating new table')\n",
    "    (new_data\n",
    "     .write\n",
    "     .mode('overwrite')\n",
    "     .format('jdbc')\n",
    "     .option('url', mf.url)\n",
    "     .option('dbtable', 'employees')\n",
    "     .option('user', mf.username)\n",
    "     .option('password', mf.password)\n",
    "     .option('driver', 'org.postgresql.Driver')\n",
    "     .save())\n",
    "# Persist the data united with previous data\n",
    "else:\n",
    "    print('Persisting data...')\n",
    "    employees = (mf.spark.read.format('jdbc').option('url', mf.url)\n",
    "                    .option('dbtable', 'employees')\n",
    "                    .option('user', mf.username)\n",
    "                    .option('password', mf.password)\n",
    "                    .option('driver', 'org.postgresql.Driver').load())\n",
    "\n",
    "    table_columns = employees.schema\n",
    "    table_columns = [(i.name, i.dataType) for i in table_columns]\n",
    "    table_columns.sort()\n",
    "\n",
    "    table_columns = {\"name\":table_columns}\n",
    "    df_table_columns = pd.DataFrame(table_columns)\n",
    "\n",
    "    different_columns = df_table_columns.merge(df_file_columns, on='name', how='outer', indicator=True).query(\"_merge not in ('both')\")\n",
    "\n",
    "    only_in_table = different_columns.query(\"_merge == 'left_only'\")['name'].tolist()\n",
    "    only_in_file = different_columns.query(\"_merge == 'right_only'\")['name'].tolist()\n",
    "\n",
    "    for column_name, column_type in only_in_table:\n",
    "        new_data = new_data.withColumn(column_name, lit(None).cast(column_type))\n",
    "\n",
    "    for column_name, column_type in only_in_file:\n",
    "        employees = employees.withColumn(column_name,lit(None).cast(column_type))\n",
    "\n",
    "    all_columns = employees.columns\n",
    "    all_columns.sort()\n",
    "\n",
    "    new_data = new_data.select(all_columns)\n",
    "    employees = employees.select(all_columns)\n",
    "\n",
    "    new_emp = ','.join([f\"'{i[0]}'\" for i in new_data.select('id').distinct().collect()])\n",
    "\n",
    "    last_change_employees = employees.filter(f\"id not in ({new_emp})\")\n",
    "\n",
    "    persist_data = last_change_employees.unionAll(new_data)\n",
    "    persist_data.write.mode('overwrite').saveAsTable('tmp_employees')\n",
    "    persist_data = mf.spark.table('tmp_employees')\n",
    "\n",
    "    (persist_data\n",
    "        .write\n",
    "        .mode('overwrite')\n",
    "        .format('jdbc')\n",
    "        .option('url', mf.url)\n",
    "        .option('dbtable', 'employees')\n",
    "        .option('user', mf.username)\n",
    "        .option('password', mf.password)\n",
    "        .option('driver', 'org.postgresql.Driver')\n",
    "        .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.rmtree_spark_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+--------------------+--------------------+----------+---------+--------------------+-------------------+-------------+--------------------+----------------+--------------------+--------+-------------+------+\n",
      "|currency|     department|               email|            fileName|        id|isDeleted|            joinedOn|     load_timestamp|         name|            position|satisfactionScoe|           startDate|  status|         type| value|\n",
      "+--------+---------------+--------------------+--------------------+----------+---------+--------------------+-------------------+-------------+--------------------+----------------+--------------------+--------+-------------+------+\n",
      "|     USD|    Engineering|alice.johnson@exa...|20230331_employee...|    abc123|    false|                null|2023-04-17 19:48:28|Alice Johnson|    Senior Developer|            null|2021-05-01T08:30:...|  Active|         Base|100000|\n",
      "|     USD|    Engineering|alice.johnson@exa...|20230331_employee...|    abc123|    false|                null|2023-04-17 19:48:28|Alice Johnson|    Senior Developer|            null|2021-05-01T08:30:...|  Active|        Bonus| 15000|\n",
      "|     USD|    Engineering|alice.johnson@exa...|20230331_employee...|    abc123|    false|                null|2023-04-17 19:48:28|Alice Johnson|    Senior Developer|            null|2021-05-01T08:30:...|  Active|     Benefits|  5000|\n",
      "|     USD|    Engineering|alice.johnson@exa...|20230331_employee...|    abc123|    false|                null|2023-04-17 19:48:28|Alice Johnson|    Senior Developer|            null|2021-05-01T08:30:...|  Active|Stock Options| 20000|\n",
      "|     USD|          Sales|john.doe@example.com|20230331_employee...|    def456|    false|                null|2023-04-17 19:48:28|     John Doe|       Sales Manager|            null|2020-01-01T09:00:...|  Active|         Base| 90000|\n",
      "|     USD|          Sales|john.doe@example.com|20230331_employee...|    def456|    false|                null|2023-04-17 19:48:28|     John Doe|       Sales Manager|            null|2020-01-01T09:00:...|  Active|        Bonus| 12000|\n",
      "|     USD|          Sales|john.doe@example.com|20230331_employee...|    def456|    false|                null|2023-04-17 19:48:28|     John Doe|       Sales Manager|            null|2020-01-01T09:00:...|  Active|     Benefits|  4000|\n",
      "|     USD|          Sales|john.doe@example.com|20230331_employee...|    def456|    false|                null|2023-04-17 19:48:28|     John Doe|       Sales Manager|            null|2020-01-01T09:00:...|  Active|   Commission| 30000|\n",
      "|     USD|Human Resources|sarah.lee@example...|20230331_employee...|    ghi789|     true|                null|2023-04-17 19:48:28|    Sarah Lee|Human Resources C...|            null|2019-08-01T08:30:...|Inactive|         Base| 75000|\n",
      "|     USD|Human Resources|sarah.lee@example...|20230331_employee...|    ghi789|     true|                null|2023-04-17 19:48:28|    Sarah Lee|Human Resources C...|            null|2019-08-01T08:30:...|Inactive|        Bonus|  8000|\n",
      "|     USD|Human Resources|sarah.lee@example...|20230331_employee...|    ghi789|     true|                null|2023-04-17 19:48:28|    Sarah Lee|Human Resources C...|            null|2019-08-01T08:30:...|Inactive|     Benefits|  3500|\n",
      "|     USD|           null|                null|20230331_employee...|abd1234rty|    false|2023-02-15T15:09:...|2023-04-17 18:35:50|    Bob Smith|             Manager|            10.5|                null|    null|         Base| 56767|\n",
      "|     USD|           null|                null|20230331_employee...|abd1234rty|    false|2023-02-15T15:09:...|2023-04-17 18:35:50|    Bob Smith|             Manager|            10.5|                null|    null|        Bonus|  5000|\n",
      "|     USD|           null|                null|20230331_employee...|abd1234rty|    false|2023-02-15T15:09:...|2023-04-17 18:35:50|    Bob Smith|             Manager|            10.5|                null|    null|     Benefits|  1000|\n",
      "+--------+---------------+--------------------+--------------------+----------+---------+--------------------+-------------------+-------------+--------------------+----------------+--------------------+--------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(mf.spark.read.format('jdbc').option('url', mf.url)\n",
    "    .option('dbtable', 'employees')\n",
    "    .option('user', mf.username)\n",
    "    .option('password', mf.password)\n",
    "    .option('driver', 'org.postgresql.Driver').load().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.rmtree_spark_directory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
